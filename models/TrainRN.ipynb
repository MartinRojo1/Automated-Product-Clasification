{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DSUmCxu27Upo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-07-31 19:47:10.953447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-07-31 19:47:10.953495: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import joblib\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "xk5_V74r65kj",
        "outputId": "94c369b7-b9bd-4cb8-a560-ca41a0a7f7bb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('DataNew.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jYprEpD7Tyj",
        "outputId": "19b6b947-3063-4392-f924-dbfb33863ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of different words through the 45232 products information: 20558\n",
            "Amount of sequences: 51007\n",
            "Sequences Examples: [3081, 1660, 27, 61, 3, 16, 115, 51, 1660, 163, 4490, 10, 2669, 26, 61] [3081, 1336, 64, 1, 27, 61, 742, 1589, 440, 4490, 10, 2669, 26, 1796, 941, 169, 148, 158, 5276] [3081, 1336, 27, 61, 3, 16, 115, 51, 1336, 163, 4490, 10, 2669, 26, 61] [1363, 1161, 27, 1336, 61, 61, 1336, 2271, 27, 27, 6290, 23] [3081, 349, 27, 61, 3, 16, 115, 51, 349, 163, 4490, 10, 2669, 26, 61]\n",
            "First Sequence padded: \n",
            " [3081 1660   27   61    3   16  115   51 1660  163 4490   10 2669   26\n",
            "   61    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n",
            "59 59 59\n",
            "Dataset Padded Shape: (51007, 59)\n"
          ]
        }
      ],
      "source": [
        "#First Step -> Tokenize and padd\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\") #num_words : the maximum number of words to keep, based on word frequency (for the 45232 products information), #oov : out-of-vocabulary token, it is agregated to words which are not in the 10000 most frequent ones\n",
        "tokenizer.fit_on_texts(df['Text']) # encoding words of information to integers, Updates internal vocabulary based on a list of sequences.\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Amount of different words through the 45232 products information:\",len(word_index))\n",
        "sequences = tokenizer.texts_to_sequences(df['Text'])\n",
        "print(\"Amount of sequences:\", len(sequences)) #Should be equal to the amount of products \n",
        "print(\"Sequences Examples:\",sequences[0], sequences[1], sequences[2], sequences[3], sequences[4]) #This is how the first 5 products info are represented, each number represent a word in \"word_index\" dict.\n",
        "padded = pad_sequences(sequences, padding=\"post\") #This takes the maxlen parameter as a referene and makes all the lists of the same lenght adding zeros on the left spaces, if nothins is pass ot will take the longest sequence as a reference\n",
        "print(\"First Sequence padded: \\n\",padded[0])\n",
        "print(len(padded[0]), len(padded[1]), len(padded[2])) #Should all be equal to \n",
        "print(\"Dataset Padded Shape:\", padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kpVoT650D8fX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B2ra7tTzD8cs"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset in train/test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded, df[df.columns[1:-1]], \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2pifAiJ77TwF"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QQG6xFt7TtG",
        "outputId": "b6162499-4f95-4b39-8feb-8d29ecd61793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "852\n",
            "20559\n",
            "59\n"
          ]
        }
      ],
      "source": [
        "num_classes = y_train.shape[1]\n",
        "print(num_classes)\n",
        "max_words = len(word_index) + 1\n",
        "print(max_words)\n",
        "maxlen = len(padded[0])\n",
        "print(maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S0k80g2EFjtg"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Activation, Flatten, Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUfDFsMp7ToP",
        "outputId": "440e01ab-f0fc-471c-876d-63da76ed7c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 59, 300)           6167700   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 59, 300)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 57, 300)           270300    \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 300)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 852)               256452    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 852)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,694,452\n",
            "Trainable params: 6,694,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 300, input_length=maxlen))\n",
        "model.add(Dropout(0.21))\n",
        "model.add(Conv1D(300, 3, padding='valid', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPool1D()) # This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs.\n",
        "model.add(Dropout(0.21))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Precision','Recall', tf.keras.metrics.AUC()])\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(),\n",
        "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "Bi60HgXUEAW6",
        "outputId": "bb129638-e26f-457f-bf8a-a0b4551c41d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "plot_model(model, to_file='model_nn_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvVJGrkFEJVd",
        "outputId": "c9b28cd2-5091-4a53-89ba-094aa70b9326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "781/781 [==============================] - 55s 69ms/step - loss: 0.0255 - precision: 0.2511 - recall: 0.1811 - auc_1: 0.8677 - val_loss: 0.0091 - val_precision: 0.8808 - val_recall: 0.4640 - val_auc_1: 0.9472 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.0073 - precision: 0.8954 - recall: 0.5495 - auc_1: 0.9579 - val_loss: 0.0053 - val_precision: 0.9149 - val_recall: 0.6806 - val_auc_1: 0.9756 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 0.0047 - precision: 0.9141 - recall: 0.7019 - auc_1: 0.9785 - val_loss: 0.0039 - val_precision: 0.9232 - val_recall: 0.7700 - val_auc_1: 0.9833 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0035 - precision: 0.9286 - recall: 0.7790 - auc_1: 0.9864 - val_loss: 0.0034 - val_precision: 0.9221 - val_recall: 0.8166 - val_auc_1: 0.9866 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 0.0028 - precision: 0.9370 - recall: 0.8222 - auc_1: 0.9907 - val_loss: 0.0031 - val_precision: 0.9250 - val_recall: 0.8337 - val_auc_1: 0.9866 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0023 - precision: 0.9455 - recall: 0.8540 - auc_1: 0.9932 - val_loss: 0.0030 - val_precision: 0.9222 - val_recall: 0.8524 - val_auc_1: 0.9871 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0019 - precision: 0.9504 - recall: 0.8747 - auc_1: 0.9955 - val_loss: 0.0029 - val_precision: 0.9226 - val_recall: 0.8611 - val_auc_1: 0.9864 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0016 - precision: 0.9557 - recall: 0.8953 - auc_1: 0.9964 - val_loss: 0.0028 - val_precision: 0.9244 - val_recall: 0.8661 - val_auc_1: 0.9855 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0014 - precision: 0.9607 - recall: 0.9105 - auc_1: 0.9975 - val_loss: 0.0029 - val_precision: 0.9236 - val_recall: 0.8728 - val_auc_1: 0.9846 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0012 - precision: 0.9625 - recall: 0.9211 - auc_1: 0.9981 - val_loss: 0.0029 - val_precision: 0.9286 - val_recall: 0.8714 - val_auc_1: 0.9833 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 0.0011 - precision: 0.9669 - recall: 0.9321 - auc_1: 0.9985 - val_loss: 0.0030 - val_precision: 0.9220 - val_recall: 0.8780 - val_auc_1: 0.9826 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "781/781 [==============================] - 48s 62ms/step - loss: 9.3627e-04 - precision: 0.9693 - recall: 0.9406 - auc_1: 0.9988 - val_loss: 0.0030 - val_precision: 0.9259 - val_recall: 0.8773 - val_auc_1: 0.9820 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "781/781 [==============================] - 48s 62ms/step - loss: 8.2721e-04 - precision: 0.9725 - recall: 0.9488 - auc_1: 0.9991 - val_loss: 0.0031 - val_precision: 0.9131 - val_recall: 0.8854 - val_auc_1: 0.9818 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 7.5010e-04 - precision: 0.9734 - recall: 0.9540 - auc_1: 0.9991 - val_loss: 0.0031 - val_precision: 0.9196 - val_recall: 0.8864 - val_auc_1: 0.9809 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 6.7756e-04 - precision: 0.9753 - recall: 0.9594 - auc_1: 0.9992 - val_loss: 0.0032 - val_precision: 0.9245 - val_recall: 0.8857 - val_auc_1: 0.9790 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "781/781 [==============================] - 48s 62ms/step - loss: 6.0261e-04 - precision: 0.9768 - recall: 0.9647 - auc_1: 0.9995 - val_loss: 0.0033 - val_precision: 0.9231 - val_recall: 0.8865 - val_auc_1: 0.9785 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 5.4775e-04 - precision: 0.9796 - recall: 0.9677 - auc_1: 0.9993 - val_loss: 0.0033 - val_precision: 0.9251 - val_recall: 0.8867 - val_auc_1: 0.9779 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 5.3409e-04 - precision: 0.9788 - recall: 0.9698 - auc_1: 0.9994 - val_loss: 0.0034 - val_precision: 0.9167 - val_recall: 0.8901 - val_auc_1: 0.9782 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 3.9645e-04 - precision: 0.9847 - recall: 0.9780 - auc_1: 0.9997 - val_loss: 0.0034 - val_precision: 0.9237 - val_recall: 0.8899 - val_auc_1: 0.9778 - lr: 1.0000e-04\n",
            "Epoch 20/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 3.4135e-04 - precision: 0.9870 - recall: 0.9813 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9270 - val_recall: 0.8895 - val_auc_1: 0.9775 - lr: 1.0000e-04\n",
            "Epoch 21/50\n",
            "781/781 [==============================] - 48s 62ms/step - loss: 3.2729e-04 - precision: 0.9879 - recall: 0.9816 - auc_1: 0.9998 - val_loss: 0.0034 - val_precision: 0.9231 - val_recall: 0.8920 - val_auc_1: 0.9778 - lr: 1.0000e-04\n",
            "Epoch 22/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 3.0958e-04 - precision: 0.9886 - recall: 0.9834 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9260 - val_recall: 0.8907 - val_auc_1: 0.9771 - lr: 1.0000e-04\n",
            "Epoch 23/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 3.0399e-04 - precision: 0.9884 - recall: 0.9836 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9278 - val_recall: 0.8893 - val_auc_1: 0.9767 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.9238e-04 - precision: 0.9892 - recall: 0.9843 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9266 - val_recall: 0.8900 - val_auc_1: 0.9766 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.8275e-04 - precision: 0.9893 - recall: 0.9848 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9254 - val_recall: 0.8910 - val_auc_1: 0.9767 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.7318e-04 - precision: 0.9898 - recall: 0.9855 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9281 - val_recall: 0.8896 - val_auc_1: 0.9758 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.6554e-04 - precision: 0.9899 - recall: 0.9863 - auc_1: 0.9999 - val_loss: 0.0034 - val_precision: 0.9283 - val_recall: 0.8900 - val_auc_1: 0.9760 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.6669e-04 - precision: 0.9897 - recall: 0.9854 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9259 - val_recall: 0.8917 - val_auc_1: 0.9764 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.5393e-04 - precision: 0.9905 - recall: 0.9864 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9277 - val_recall: 0.8907 - val_auc_1: 0.9761 - lr: 1.0000e-05\n",
            "Epoch 30/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.5401e-04 - precision: 0.9910 - recall: 0.9865 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8908 - val_auc_1: 0.9761 - lr: 1.0000e-05\n",
            "Epoch 31/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.5083e-04 - precision: 0.9903 - recall: 0.9872 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9277 - val_recall: 0.8907 - val_auc_1: 0.9760 - lr: 1.0000e-05\n",
            "Epoch 32/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.4620e-04 - precision: 0.9906 - recall: 0.9873 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9288 - val_recall: 0.8901 - val_auc_1: 0.9758 - lr: 1.0000e-05\n",
            "Epoch 33/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.5164e-04 - precision: 0.9909 - recall: 0.9867 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9275 - val_recall: 0.8910 - val_auc_1: 0.9760 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.4640e-04 - precision: 0.9908 - recall: 0.9867 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9277 - val_recall: 0.8908 - val_auc_1: 0.9760 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "781/781 [==============================] - 49s 62ms/step - loss: 2.4005e-04 - precision: 0.9905 - recall: 0.9875 - auc_1: 1.0000 - val_loss: 0.0035 - val_precision: 0.9285 - val_recall: 0.8903 - val_auc_1: 0.9758 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.4355e-04 - precision: 0.9909 - recall: 0.9872 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9276 - val_recall: 0.8905 - val_auc_1: 0.9760 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 2.4382e-04 - precision: 0.9908 - recall: 0.9873 - auc_1: 1.0000 - val_loss: 0.0035 - val_precision: 0.9275 - val_recall: 0.8905 - val_auc_1: 0.9760 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 2.4526e-04 - precision: 0.9905 - recall: 0.9869 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9279 - val_recall: 0.8903 - val_auc_1: 0.9759 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "781/781 [==============================] - 53s 68ms/step - loss: 2.4248e-04 - precision: 0.9903 - recall: 0.9871 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9279 - val_recall: 0.8903 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "781/781 [==============================] - 53s 67ms/step - loss: 2.3867e-04 - precision: 0.9908 - recall: 0.9875 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9279 - val_recall: 0.8904 - val_auc_1: 0.9760 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "781/781 [==============================] - 51s 66ms/step - loss: 2.4105e-04 - precision: 0.9911 - recall: 0.9870 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9279 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "781/781 [==============================] - 51s 65ms/step - loss: 2.3927e-04 - precision: 0.9909 - recall: 0.9873 - auc_1: 1.0000 - val_loss: 0.0035 - val_precision: 0.9279 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "781/781 [==============================] - 52s 67ms/step - loss: 2.4226e-04 - precision: 0.9909 - recall: 0.9874 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8904 - val_auc_1: 0.9760 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "781/781 [==============================] - 53s 68ms/step - loss: 2.3777e-04 - precision: 0.9909 - recall: 0.9875 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9277 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "781/781 [==============================] - 52s 67ms/step - loss: 2.4094e-04 - precision: 0.9912 - recall: 0.9865 - auc_1: 1.0000 - val_loss: 0.0035 - val_precision: 0.9276 - val_recall: 0.8904 - val_auc_1: 0.9760 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "781/781 [==============================] - 47s 61ms/step - loss: 2.4262e-04 - precision: 0.9904 - recall: 0.9878 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9277 - val_recall: 0.8904 - val_auc_1: 0.9760 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "781/781 [==============================] - 57s 73ms/step - loss: 2.3953e-04 - precision: 0.9908 - recall: 0.9874 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "781/781 [==============================] - 54s 69ms/step - loss: 2.3850e-04 - precision: 0.9909 - recall: 0.9875 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "781/781 [==============================] - 55s 70ms/step - loss: 2.4764e-04 - precision: 0.9907 - recall: 0.9867 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-07\n",
            "Epoch 50/50\n",
            "781/781 [==============================] - 52s 67ms/step - loss: 2.4692e-04 - precision: 0.9909 - recall: 0.9868 - auc_1: 0.9999 - val_loss: 0.0035 - val_precision: 0.9278 - val_recall: 0.8904 - val_auc_1: 0.9759 - lr: 1.0000e-07\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.3,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q7XT7_A4WKGf"
      },
      "outputs": [],
      "source": [
        "def get_standar_metrics(predictions, y_test):\n",
        "  a = tranform_binary(predictions)\n",
        "  print(\"Evaluation Metrics\")\n",
        "  print('-'*30)\n",
        "  print(\"F1 Score:\",metrics.f1_score(y_test, a, average=\"micro\"))\n",
        "  print(\"Recall Score:\",metrics.recall_score(y_test, a, average=\"micro\"))\n",
        "  print(\"Precision Score:\",metrics.precision_score(y_test, a, average=\"micro\"))\n",
        "  print(\"Hamming Loss:\",metrics.hamming_loss(y_test, a), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n6wiLjzDWWCd"
      },
      "outputs": [],
      "source": [
        "def tranform_binary(predictions):\n",
        "    a = predictions.tolist()\n",
        "    for i in range(len(a)):\n",
        "        for j in range(len(a[i])):\n",
        "            if a[i][j] >= 0.5:\n",
        "                a[i][j] = 1\n",
        "            else:\n",
        "                a[i][j] = 0\n",
        "\n",
        "    a = np.array(a)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "JAqYdUXUVrS5",
        "outputId": "e8881ba7-425a-4ac5-b035-3dbfc47e3a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "479/479 [==============================] - 2s 4ms/step\n",
            "Evaluation Metrics\n",
            "------------------------------\n",
            "F1 Score: 0.9058894960534305\n",
            "Recall Score: 0.8814301030566921\n",
            "Precision Score: 0.9317451139123395\n",
            "Hamming Loss: 0.000641961946152508 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Model Evaluation with Standar Metrics\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "get_standar_metrics(predictions, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "modeloF =  keras.models.load_model('model-conv1d.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 963 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5635a2a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 85ms/step\n"
          ]
        }
      ],
      "source": [
        "inp = 'metra radio dash multikit select gm vehicle black expand online assortment compatible select gm vehicle plastic material'\n",
        "\n",
        "x = tokenizer.texts_to_sequences([inp])\n",
        "\n",
        "x = pad_sequences(x)\n",
        "\n",
        "pre = modeloF.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tokenizer.plk']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "joblib.dump(tokenizer,'tokenizer.plk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "predScores = [score for pred in pre for score in pred]\n",
        "\n",
        "predDict = {}\n",
        "for cla,score in zip(df.columns[1:],predScores):\n",
        "        predDict[cla] = score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Car Electronics & GPS', 0.9999984),\n",
              " ('Car Audio Installation Parts', 0.9999419),\n",
              " ('Car Installation Parts & Accessories', 0.99992317),\n",
              " ('Back-Up & Dash Cameras', 0.02803315),\n",
              " ('Smartphone & iPod Car Connectors', 0.021082692)]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import operator\n",
        "sorted(predDict.items(), key=operator.itemgetter(1),reverse=True)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ModelPrediction(text):\n",
        "\n",
        "    tokenizer = joblib.load('tokenizer.plk')\n",
        "\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    sequence = pad_sequences(x,maxlen=59)\n",
        "\n",
        "    model = keras.models.load_model('model-conv1d.h5')\n",
        "\n",
        "    prediction = model.predict(sequence)\n",
        "    \n",
        "    predScores = [score for pred in prediction for score in pred]\n",
        "\n",
        "    predDict = {}\n",
        "\n",
        "    for cla,score in zip(df.columns[1:],predScores):\n",
        "        \n",
        "        predDict[cla] = score\n",
        "    \n",
        "    preSort = sorted(predDict.items(), key=operator.itemgetter(1),reverse=True)[:3]\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in preSort:\n",
        "        \n",
        "        if i[1] > 0.80:\n",
        "            \n",
        "            result.append(i[0])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 44ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Car Electronics & GPS',\n",
              " 'Car Audio Installation Parts',\n",
              " 'Car Installation Parts & Accessories']"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ModelPrediction(inp)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
